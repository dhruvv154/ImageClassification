# -*- coding: utf-8 -*-
"""Transfer Learning (Research).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NdYYppxAd7av_aj_Lv2bdo3f1N8acvhq
"""

import torch
import torch.nn as nn

x=torch.tensor([2,3])
y=torch.tensor([4,6])

print(x.shape)

import numpy as np

x=np.zeros((2,2))
print(x)

y=np.ones((3,3))
print(y)

z=np.random.random((3,4))
print(z)

import numpy as np

a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])
b = a[:2, 1:3]
print(b)

import torch

x=torch.arange(12, dtype=torch.float32)
print(x)

x=x.reshape(3,4)
torch.exp(x)

print(x)

import os

os.makedirs(os.path.join('..', 'data'), exist_ok=True)
data_file = os.path.join('..', 'data', 'house_tiny.csv')
with open(data_file, 'w') as f:
    f.write('''NumRooms,RoofType,Price
NA,NA,127500
2,NA,106000
4,Slate,178100
NA,NA,140000''')

import pandas as pd

data = pd.read_csv(data_file)
print(data)

import torch
import torch.nn as nn

sigmoid = nn.Sigmoid()

x = torch.tensor([-100, 0.0, 200.0])
output = sigmoid(x)
print(output)

import torch
import torch.nn as nn

# Sample Data: x → y
x = torch.tensor([[1.0], [2.0], [3.0], [4.0]])
y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])

# Model
model = nn.Linear(1, 1)

# Loss and Optimizer
loss_fn = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# Training
for epoch in range(100):
    y_pred = model(x)
    loss = loss_fn(y_pred, y)
    loss.backward()           # Computes gradients
    optimizer.step()          # Updates weights using gradient descent
    optimizer.zero_grad()     # Clears gradients
    if (epoch+1) % 10 == 0:
        print(f'Epoch {epoch+1}: Loss = {loss.item():.4f}')

import torch
import torch.nn.functional as F

x = torch.linspace(-5, 5, 100)

sigmoid = torch.sigmoid(x)
tanh = torch.tanh(x)
relu = F.relu(x)

# Plot (requires matplotlib)
import matplotlib.pyplot as plt
plt.plot(x.numpy(), sigmoid.numpy(), label='Sigmoid')
plt.plot(x.numpy(), tanh.numpy(), label='Tanh')
plt.plot(x.numpy(), relu.numpy(), label='ReLU')
plt.legend(); plt.title("Activation Functions"); plt.grid(); plt.show()

from torchvision import models, transforms
from torch import nn

# Load pretrained model
model = models.resnet18(pretrained=True)
for param in model.parameters():
    param.requires_grad = False  # Freeze layers

# Replace classifier for 2-class classification
model.fc = nn.Linear(model.fc.in_features, 2)

"""start here"""

import torch
import torch.nn as nn

# Sample data
X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])
Y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])

# Model
model = nn.Linear(1, 1)

# Loss and Optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# Training loop
for epoch in range(100):
    y_pred = model(X)
    loss = criterion(y_pred, Y)

    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    if (epoch+1) % 10 == 0:
        print(f'Epoch {epoch+1}: Loss = {loss.item():.4f}')

import torch

x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])

# Sigmoid
print("Sigmoid:", torch.sigmoid(x))

# Tanh
print("Tanh:", torch.tanh(x))

# ReLU
print("ReLU:", torch.relu(x))

import torch
import torch.nn as nn

# Dummy values
y_true = torch.tensor([1])
y_pred = torch.tensor([[2.0, 0.5]])  # class scores

# Cross-entropy loss for classification
loss_fn = nn.CrossEntropyLoss()
loss = loss_fn(y_pred, y_true)

print(f'Loss: {loss.item()}')

import torch.nn as nn

model = nn.Sequential(
    nn.Linear(100, 50),
    nn.BatchNorm1d(50), #BatchNorm2d for convolutional layers
    nn.ReLU(),
    nn.Linear(50, 10)
)

from torchvision import models
import torch.nn as nn

# Load a pre-trained model
model = models.resnet18(pretrained=True)

# Freeze all layers
for param in model.parameters():
    param.requires_grad = False

# Replace the final classification layer
model.fc = nn.Linear(model.fc.in_features, 2)  # 2 classes for your new task

import torch
from torchvision import models
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)

"""START HERE!!!"""



"""True Positive (TP): The model correctly predicted a positive instance as positive.
True Negative (TN): The model correctly predicted a negative instance as negative.
False Positive (FP): The model incorrectly predicted a negative instance as positive (also known as a Type I error).
False Negative (FN): The model incorrectly predicted a positive instance as negative (also known as a Type II error).
"""





#Image Classification Metrics

# Import necessary libraries
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt


# Prepare dummy data

# y_true: the actual class labels
# y_pred: the predicted class labels by your model
y_true = np.array([0, 1, 2, 2, 0, 1, 1, 2, 0, 2])
y_pred = np.array([0, 2, 1, 2, 0, 1, 1, 2, 0, 1])


# Step 2: Calculate metrics


# Accuracy: how many predictions were correct
accuracy = accuracy_score(y_true, y_pred)

# Precision: out of all predicted classes, how many were correct
# 'macro' means it computes precision per class and then averages
precision = precision_score(y_true, y_pred, average='macro')

# Recall: out of all actual classes, how many were correctly predicted
recall = recall_score(y_true, y_pred, average='macro')

# F1 Score: harmonic mean of precision and recall
f1 = f1_score(y_true, y_pred, average='macro')

# Confusion Matrix: shows actual vs predicted for each class
conf_matrix = confusion_matrix(y_true, y_pred)


print("Image Classification Metrics")
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision (macro): {precision:.2f}")
print(f"Recall (macro): {recall:.2f}")
print(f"F1 Score (macro): {f1:.2f}")


# This shows how well the model predicted each class
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, cmap="Blues", fmt="d")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

"""precision measures the accuracy of positive predictions, while recall measures the model's ability to find all relevant instances"""

#  Object Detection Metrics Implementation


import numpy as np
import matplotlib.pyplot as plt


# IoU calculator between two boxes


def compute_iou(box1, box2):
    """
    box1, box2: [x1, y1, x2, y2]
    Coordinates of top-left and bottom-right corners
    """
    xA = max(box1[0], box2[0])
    yA = max(box1[1], box2[1])
    xB = min(box1[2], box2[2])
    yB = min(box1[3], box2[3])

    # Compute area of intersection rectangle
    interArea=max(0, xB - xA) * max(0, yB - yA)

    # Compute area of both bounding boxes
    box1Area=(box1[2] - box1[0]) * (box1[3] - box1[1])
    box2Area=(box2[2] - box2[0]) * (box2[3] - box2[1])

    # IoU=intersection / union
    iou = interArea / float(box1Area + box2Area - interArea)
    return iou


# Evaluate dummy predictions vs ground truth


# One ground truth box
gt_boxes = [[50, 50, 150, 150]]  # Class: person

# Predicted boxes: format → [x1, y1, x2, y2, confidence]
pred_boxes = [
    [55, 55, 148, 148, 0.9],   # high overlap
    [30, 30, 70, 70, 0.6],     # low overlap
    [60, 60, 120, 120, 0.8]    # decent overlap
]

# IoU threshold for "True Positive"
iou_threshold = 0.5

# Step 3: Sort predictions by confidence
pred_boxes = sorted(pred_boxes, key=lambda x: x[4], reverse=True)

# Step 4: Match predictions to GT
TP = []
FP = []
used = set()

for pred in pred_boxes:
    pred_box = pred[:4]
    best_iou = 0
    best_gt_idx = -1
    for i, gt in enumerate(gt_boxes):
        iou = compute_iou(pred_box, gt)
        if iou > best_iou:
            best_iou = iou
            best_gt_idx = i

    if best_iou >= iou_threshold and best_gt_idx not in used:
        TP.append(1)
        FP.append(0)
        used.add(best_gt_idx)
    else:
        TP.append(0)
        FP.append(1)

# Step 5: Compute Precision and Recall
TP_cumsum = np.cumsum(TP)
FP_cumsum = np.cumsum(FP)
recall = TP_cumsum / len(gt_boxes)
precision = TP_cumsum / (TP_cumsum + FP_cumsum + 1e-6)

# Step 6: Plot Precision-Recall curve
plt.plot(recall, precision, marker='o')
plt.title("Precision-Recall Curve")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.grid(True)
plt.show()

# Step 7: Compute AP (area under curve)
AP = np.trapz(precision, recall)
print(f"Average Precision (AP): {AP:.2f}")

import numpy as np


 #Dummy segmentation masks

# Ground truth mask (true class labels for each pixel)
true_mask = np.array([
    [0, 1, 1],
    [1, 2, 2],
    [0, 0, 2]
])

# Predicted mask (model's output)
pred_mask = np.array([
    [0, 1, 0],
    [1, 2, 2],
    [0, 1, 2]
])


#  Pixel Accuracy Function

def pixel_accuracy(pred_mask, true_mask):
    """
    Compute pixel accuracy = correct predictions / total pixels.
    """
    assert pred_mask.shape == true_mask.shape, "Shape mismatch!"
    correct_pixels = (pred_mask == true_mask).sum()
    total_pixels = true_mask.size
    return correct_pixels / total_pixels

# Step 3: Run and Print Result

accuracy = pixel_accuracy(pred_mask, true_mask)
print(f"Pixel Accuracy: {accuracy:.4f}")

import numpy as np


#Create masks (binary)


# 1means object, 0 means background
gt_mask = np.array([
    [0, 0, 1, 1],
    [0, 1, 1, 1],
    [0, 0, 1, 0],
    [0, 0, 0, 0]
])

# Predicted Mask: model's guess
pred_mask = np.array([
    [0, 1, 1, 1],
    [0, 1, 1, 0],
    [0, 0, 1, 0],
    [0, 0, 0, 0]
])


# Step 2: Compute IoU

def compute_iou(mask1, mask2):
    intersection = np.logical_and(mask1, mask2).sum()
    union = np.logical_or(mask1, mask2).sum()
    iou = intersection / union
    return iou


# Step 3: Compute Dice Score

def compute_dice(mask1, mask2):
    intersection = np.logical_and(mask1, mask2).sum()
    dice = 2 * intersection / (mask1.sum() + mask2.sum())
    return dice

iou = compute_iou(gt_mask, pred_mask)
dice = compute_dice(gt_mask, pred_mask)

print(f"IoU: {iou:.4f}")
print(f"Dice Score: {dice:.4f}")



import numpy as np


# Step 1: Simulated Keypoints
# Format: [(x, y), ...] for each joint
# Example: 5 keypoints (nose, L shoulder, R shoulder, L wrist, R wrist)

gt_keypoints = np.array([
    [100, 100],   # nose
    [90, 130],    # left shoulder
    [110, 130],   # right shoulder
    [85, 170],    # left wrist
    [115, 170]    # right wrist
])

pred_keypoints = np.array([
    [102, 98],    # nose (correct)
    [95, 128],    # left shoulder (correct)
    [115, 140],   # right shoulder (too far)
    [84, 168],    # left wrist (correct)
    [120, 165]    # right wrist (too far)
])


# PCK Function


def compute_pck(gt, pred, threshold=10):

    distances = np.linalg.norm(gt - pred, axis=1)
    correct = distances < threshold
    pck = correct.sum() / len(gt)
    return pck


# Step 3: Run and Display

threshold = 10  # pixels
pck_value = compute_pck(gt_keypoints, pred_keypoints, threshold)
print(f"PCK@{threshold}px: {pck_value:.2f}")

import numpy as np
from sklearn.metrics import roc_auc_score


#Simulated Embeddings


# 5 pairs: (embedding1, embedding2), with labels: 1 = same person, 0 = different
np.random.seed(0)
embedding_pairs = [(np.random.rand(128), np.random.rand(128)) for _ in range(5)]

#ground truth whether the pairs match
labels = np.array([1, 0, 1, 0, 1])  # 3 matches, 2 non-matches


# Step 2: Cosine Similarity Function
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


# Step 3: Compute Similarities & Predictions

similarities = np.array([cosine_similarity(a, b) for a, b in embedding_pairs])
threshold = 0.8
predictions = (similarities > threshold).astype(int)


# Step 4: Metrics
TP = ((predictions == 1) & (labels == 1)).sum()
TN = ((predictions == 0) & (labels == 0)).sum()
FP = ((predictions == 1) & (labels == 0)).sum()
FN = ((predictions == 0) & (labels == 1)).sum()
accuracy = (TP + TN) / len(labels)
FAR = FP / (FP + TN + 1e-6)
FRR = FN / (FN + TP + 1e-6)
roc_auc = roc_auc_score(labels, similarities)
print(f"Accuracy: {accuracy:.2f}")
print(f"FAR (False Acceptance Rate): {FAR:.2f}")
print(f"FRR (False Rejection Rate): {FRR:.2f}")
print(f"ROC-AUC: {roc_auc:.2f}")

from jiwer import cer, wer


# Step 1: Simulated OCR Results

ground_truth = "The quick brown fox jumps over the lazy dog"
predicted_text = "The quik brown foks jump over the lazi dog"

# Step 2: Compute CER and WER

char_error = cer(ground_truth, predicted_text)
word_error = wer(ground_truth, predicted_text)

print(f"Character Error Rate (CER): {char_error:.2f}")
print(f"Word Error Rate (WER): {word_error:.2f}")

#image captioning

!pip install transformers
!pip install sentencepiece
!pip install evaluate
!pip install torchvision

from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import requests

# Load BLIP model and processor
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Load a sample image from web
img_url = "https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"
image = Image.open(requests.get(img_url, stream=True).raw).convert("RGB")

# Preprocess image and generate caption
inputs = processor(image, return_tensors="pt")
output = model.generate(**inputs)
generated_caption = processor.decode(output[0], skip_special_tokens=True)

print(" Generated Caption:", generated_caption)

import evaluate

# Ground truth (you can change this based on what you think the image is)
ground_truth = ["two parrots perched on a tree branch"]
predicted = [generated_caption]

# Load metrics
bleu = evaluate.load("bleu")
meteor = evaluate.load("meteor")
rouge = evaluate.load("rouge")

# Evaluate
print("BLEU:", bleu.compute(predictions=predicted, references=ground_truth))
print("METEOR:", meteor.compute(predictions=predicted, references=ground_truth))
print("ROUGE:", rouge.compute(predictions=predicted, references=ground_truth))

#image super resolution

!pip install scikit-image
!pip install torchvision

from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim
from PIL import Image
import numpy as np
import torchvision.transforms as T


# Step 1: Load and prepare image

url = "https://upload.wikimedia.org/wikipedia/commons/4/47/PNG_transparency_demonstration_1.png"
img = Image.open(requests.get(url, stream=True).raw).convert("RGB")

# Resize to high-res
hr = img.resize((256, 256))
lr = hr.resize((64, 64))  # simulate low-res
sr = lr.resize((256, 256))  # simulate super-res

# Convert to numpy arrays
hr_np = np.array(hr)
sr_np = np.array(sr)

# ----------------------------
# Step 2: Compute Metrics
# ----------------------------
psnr_val = psnr(hr_np, sr_np)
ssim_val = ssim(hr_np, sr_np, channel_axis=2)

print(f"PSNR: {psnr_val:.2f} dB")
print(f"SSIM: {ssim_val:.3f}")

# a bit confused on image generation coding wise

import numpy as np


#Simulate depth maps


# Ground truth depth (e.g., from LIDAR or stereo)
depth_gt = np.array([
    [1.0, 2.0, 3.0],
    [2.5, 4.0, 3.5],
    [1.5, 2.0, 2.5]
])

# Predicted depth (from model)
depth_pred = np.array([
    [1.1, 1.8, 2.9],
    [2.4, 4.2, 3.7],
    [1.6, 2.1, 2.4]
])


#Compute Metrics


def compute_rmse(gt, pred):
    return np.sqrt(np.mean((gt - pred) ** 2))

def compute_abs_rel(gt, pred):
    return np.mean(np.abs(gt - pred) / gt)

def compute_delta_accuracy(gt, pred, threshold):
    delta = np.maximum(gt / pred, pred / gt)
    return np.mean(delta < threshold)



rmse = compute_rmse(depth_gt, depth_pred)
abs_rel = compute_abs_rel(depth_gt, depth_pred)
acc1 = compute_delta_accuracy(depth_gt, depth_pred, 1.25)
acc2 = compute_delta_accuracy(depth_gt, depth_pred, 1.25 ** 2)
acc3 = compute_delta_accuracy(depth_gt, depth_pred, 1.25 ** 3)

print(f"RMSE: {rmse:.3f}")
print(f" Abs Rel Error: {abs_rel:.3f}")

import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score


# Let's say we have 5 video samples and 6 action classes

y_true = np.array([2, 0, 1, 3, 4])  # ground truth labels
y_pred_probs = np.array([
    [0.1, 0.1, 0.6, 0.1, 0.05, 0.05],  # correct (2)
    [0.3, 0.2, 0.1, 0.05, 0.25, 0.1],  # correct (0)
    [0.1, 0.6, 0.2, 0.05, 0.025, 0.025],  # correct (1)
    [0.2, 0.2, 0.1, 0.25, 0.15, 0.1],  # correct (3)
    [0.05, 0.2, 0.2, 0.1, 0.4, 0.05],  # correct (4)
])


#Compute Top-k Accuracy

top1_preds = np.argmax(y_pred_probs, axis=1)
top5_preds = np.argsort(y_pred_probs, axis=1)[:, -5:]

top1_acc = np.mean(top1_preds == y_true)
top5_acc = np.mean([y_true[i] in top5_preds[i] for i in range(len(y_true))])


# Precision / Recall / F1

precision = precision_score(y_true, top1_preds, average='macro')
recall = recall_score(y_true, top1_preds, average='macro')
f1 = f1_score(y_true, top1_preds, average='macro')


print(f" Top-1 Accuracy: {top1_acc:.2f}")
print(f" Top-5 Accuracy: {top5_acc:.2f}")
print(f" Precision: {precision:.2f}")
print(f" Recall: {recall:.2f}")
print(f" F1 Score: {f1:.2f}")

